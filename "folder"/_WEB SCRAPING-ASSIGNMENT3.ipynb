{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670c8da4",
   "metadata": {},
   "source": [
    "# WEB SCRAPING-ASSIGNMENT3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5c9d5",
   "metadata": {},
   "source": [
    "Write a python program which searches all the product under a particular product from www.amazon.in. The \n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for \n",
    "guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf6a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\shaik\\anaconda3\\lib\\site-packages (4.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from selenium) (0.26.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.1)\n",
      "Requirement already satisfied: idna in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\shaik\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1ab4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65853242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(2)\n",
    "\n",
    "# Opening Amazon.in in chrome browser\n",
    "url='http://www.amazon.in'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# Taking input from user about product search\n",
    "User_input=input('Enter the title of Product you are interest in search :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_xpath('//input[@id=\"twotabsearchtextbox\"]')\n",
    "# clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input specified by user to search menu through send keys\n",
    "Search.send_keys(User_input)\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_xpath('//input[@id=\"nav-search-submit-button\"]')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ed873",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bef8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Empty list to scrap data\n",
    "Brand =[]\n",
    "Product = []\n",
    "Rating =[]\n",
    "No_of_ratings =[]\n",
    "Price =[]\n",
    "Return =[]\n",
    "Excepted_delivery =[]\n",
    "Availability =[]\n",
    "Other_details =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec79d6",
   "metadata": {},
   "source": [
    "Scraping url of all product listed on first 3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eede536",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL= []\n",
    "# range(0,3) used to scrape three pages on website\n",
    "for page in range(0,3):\n",
    "    url=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for i in url:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    time.sleep(2)\n",
    "    # locating next page button and clicking\n",
    "    Nxt_page=driver.find_element_by_xpath('//li[@class=\"a-last\"][1]/a')\n",
    "    Nxt_page.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extracting Brand Name via Xpath\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')\n",
    "        Brand.append(brand.text) \n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "        \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting product name via Xpath\n",
    "    try:\n",
    "        product =driver.find_element_by_xpath('//span[@id=\"productTitle\"]')\n",
    "        Product.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        Product.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Rating via Xpath\n",
    "    try:\n",
    "        rating=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-base\"]')\n",
    "        Rating.append(rating.text)\n",
    "    except NoSuchElementException:\n",
    "        Rating.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting No of Ratings via Xpath\n",
    "    try:\n",
    "        rating_count=driver.find_element_by_xpath('//span[@class=\"a-size-base a-color-secondary\"]')\n",
    "        No_of_ratings.append(rating_count.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_ratings.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Price via Xpath\n",
    "    try:\n",
    "        price=driver.find_element_by_xpath('//span[@id=\"priceblock_dealprice\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Return or exchange detail via Xpath\n",
    "    try:\n",
    "        replacement=driver.find_element_by_xpath('//*[@id=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        Return.append(replacement.text)\n",
    "    except NoSuchElementException:\n",
    "        Return.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Expected Delivery via Xpath\n",
    "    try:\n",
    "        delivery=driver.find_element_by_xpath('//div[@id=\"ddmDeliveryMessage\"]/b')\n",
    "        Excepted_delivery.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        Excepted_delivery.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Availability via Xath\n",
    "    try:\n",
    "        availability=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-success\"]')\n",
    "        Availability.append(availability.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting other details via Xpath\n",
    "    try:\n",
    "        other=driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')\n",
    "        Other_details.append(other.text)\n",
    "    except NoSuchElementException:\n",
    "        Other_details.append('-')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Guitar=pd.DataFrame({'Brand':Brand,'Product':Product,'Rating':Rating,'No. of ratings':No_of_ratings,'Price':Price,\n",
    "                        'Return/Exchange':Return,'Expected Delivery':Excepted_delivery,'Availability':Availability,\n",
    "                        'Other Details':Other_details,'URL':URL})\n",
    "print('\\033[1m'+'Amazon Festive Sale Guitar with exciting offers :'+'\\033[0m')\n",
    "Guitar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02edf6a",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b244ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in Chrome browser\n",
    "url='https://images.google.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886d81",
   "metadata": {},
   "source": [
    "Searching and extracting for Fruits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Fruits' in search bar\n",
    "Search.send_keys('Fruits')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_class_name('zgAlFc')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0045441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 25000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,25000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Infinity\\Fliprobbo\\WebScraping Assignment 3 Selenium\\Fruits\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c98a54",
   "metadata": {},
   "source": [
    "Searching and extracting for Cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dfa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Cars' in search bar\n",
    "Search.send_keys('Cars')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_class_name('zgAlFc')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 25000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,50000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca203291",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1cba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Infinity\\Fliprobbo\\WebScraping Assignment 3 Selenium\\Cars\" +str(i)+ \".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb02fd7",
   "metadata": {},
   "source": [
    "Searching and extracting for Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in Chrome browser\n",
    "url='https://images.google.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11525340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by class\n",
    "Search=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Machine Learning' in search bar\n",
    "Search.send_keys('Machine Learning')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_class_name('zgAlFc')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 50000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,50000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1492fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "images= driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            URL.append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df436c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Infinity\\Fliprobbo\\WebScraping Assignment 3 Selenium\\Machine Learning\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e727de7",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com \n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand \n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,  \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing require libary\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in Chrome browser\n",
    "url='http://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    login_window = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "    login_window.click()\n",
    "except NoSuchElementException:\n",
    "    print('Login Window is not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Search bar website by xpath\n",
    "Search=driver.find_element_by_xpath('//div[@class=\"_3OO5Xc\"]/input')\n",
    "# Clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input 'Oneplus' in search bar\n",
    "Search.send_keys('Iphone Mobile')\n",
    "# Finding Search button for clicking through class name\n",
    "Search_button=driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making empty list to scrape data\n",
    "Brand =[]\n",
    "Smartphone =[]\n",
    "Colour =[]\n",
    "Storage_Rom =[]\n",
    "Primary_camera=[]\n",
    "Secondary_camera=[]\n",
    "Display_size=[]\n",
    "Display_resolution=[]\n",
    "Processor =[]\n",
    "Processor_cores=[]\n",
    "Battery_capacity=[]\n",
    "Price =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f718b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=[]\n",
    "url=driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for i in url:\n",
    "    Href=i.get_attribute('href')\n",
    "    URL.append(Href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Expanding specification table by clicking on read more button\n",
    "    try:\n",
    "        Read_more=driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')\n",
    "        Read_more.click()\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        print('NoSuchElementException Occur')\n",
    "        pass\n",
    "    \n",
    "    # Extracting Brand Name via Xpath\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')\n",
    "        Brand.append(brand.text.split()[0])\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "        \n",
    "    # Extracting Smartphone Model via Xpath\n",
    "    try:\n",
    "        smartphone=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        Smartphone.append(smartphone.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Smartphone.append('-')\n",
    "        \n",
    "    # Extracting Colour via Xpath\n",
    "    try:\n",
    "        colour=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        Colour.append(colour.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Colour.append('-')\n",
    "    \n",
    "    # Extracting Storage Rom via Xpath\n",
    "    try:\n",
    "        Rom=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Storage_Rom.append(Rom.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Storage_Rom.append('Not Mention')\n",
    "        \n",
    "    # Extracting primary camera detail via Xpath\n",
    "    try:\n",
    "        primary_camera=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Primary_camera.append(primary_camera.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Primary_camera.append('Not Mention')\n",
    "        \n",
    "    # Extracting Secondary Camera detail via Xpath\n",
    "    try:\n",
    "        secondary_cam=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table/tbody/tr[5]/td[2]/ul/li')\n",
    "        Secondary_camera.append(secondary_cam.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Secondary_camera.append('Not Mention')\n",
    "    \n",
    "    # Extracting Display size via Xpath\n",
    "    try:\n",
    "        display=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Display_size.append(display.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_size.append('-')\n",
    "    \n",
    "    # Extracting Display Resolution via Xpath\n",
    "    try:\n",
    "        resolution=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Display_resolution.append(resolution.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_resolution.append('-')\n",
    "    \n",
    "    # Extracting processor detail via xpath\n",
    "    try:\n",
    "        processor=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Processor.append(processor.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        Processor.append('-')\n",
    "        \n",
    "    # Extracting Processor core via xpath\n",
    "    try:\n",
    "        core=driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table/tbody/tr[3]/td[2]/ul/li')\n",
    "        Processor_cores.append(core.text)\n",
    "    except NoSuchElementException:\n",
    "        Processor_cores.append('Not Mention')\n",
    "    \n",
    "    #Extracting Price via X path\n",
    "    try:\n",
    "        price=driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('Not Mention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iphone=pd.DataFrame({'Brand':Brand,'Smartphone':Smartphone,'Colour':Colour,'Price':Price,\n",
    "                        'Storage Rom':Storage_Rom,'Primary Camera':Primary_camera,'Display Size':Display_size,\n",
    "                        'Display Resolution':Display_resolution,'Processor':Processor,'Processor Cores':Processor_cores})\n",
    "print('\\033[1m'+'Flipkart Iphone Models :'+'\\033[0m')\n",
    "Iphone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57807f5",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03960806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.google.co.in/maps'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_id(\"searchboxinput\") \n",
    "# clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input specified by user to search menu through send keys\n",
    "Search.send_keys('Nashik')\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_id(\"searchbox-searchbutton\")  \n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_url=driver.current_url\n",
    "print('Current url :',current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if \"@\" in current_url:\n",
    "        location=current_url.split('@')[1].split(',*/data')[0].split(',')\n",
    "        location\n",
    "        print('Latitude of given Location:',location[0])\n",
    "        print('Longitude of given Location:',location[1])\n",
    "except:\n",
    "    print('Location detail not found in url')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11db448",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.digit.in'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf22694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on top 10 option button in menu bar\n",
    "driver.find_element_by_xpath('//div[@class=\"menu\"]/ul/li[4]/a').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Clicking on Laptop under table\n",
    "driver.find_element_by_xpath('//div[@class=\"categoty_list\"]/button[2]').click()\n",
    "\n",
    "# Opening Best gaming laptops link\n",
    "Gaming_lappy=driver.find_element_by_xpath('//div[@id=\"laptops\"]/div[3]/a').get_attribute('href')\n",
    "driver.get(Gaming_lappy)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Empty list for scraping data\n",
    "Laptop_model =[]\n",
    "OS =[]\n",
    "Display = []\n",
    "Processor =[]\n",
    "Memory =[]\n",
    "Weight =[]\n",
    "Dimension =[]\n",
    "Graphics_processor =[]\n",
    "Price =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop model via Xpath\n",
    "for i in range(0,11):\n",
    "    laptop_model=driver.find_elements_by_xpath('//*[@id=\"toptenIdevent{}\"]/a/h3'.format(i))\n",
    "    for j in laptop_model:\n",
    "       Laptop_model.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting OS on Laptop model via Xpath\n",
    "os=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[3]/td[3]')\n",
    "for j in os:\n",
    "    OS.append(j.text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278fd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Display vertical via Xpath\n",
    "display=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[4]/td[3]')\n",
    "for j in display:\n",
    "    Display.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e43044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Processor via xpath\n",
    "processor=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[5]/td[3]')\n",
    "for j in processor:\n",
    "    Processor.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Memory via xpath\n",
    "memory=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[6]/td[3]')\n",
    "for j in memory:\n",
    "    Memory.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop Weight via xpath\n",
    "weight=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[7]/td[3]')\n",
    "for j in weight:\n",
    "    Weight.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fe45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Dimension via xpath\n",
    "dimension=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[8]/td[3]')\n",
    "for j in dimension:\n",
    "    Dimension.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Graphics Processor type via xpath\n",
    "graphics_processor=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[9]/td[3]')\n",
    "for j in graphics_processor:\n",
    "    Graphics_processor.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop Price type via xpath\n",
    "price=driver.find_elements_by_xpath('//table[@id=\"summtable\"]//tr//td[3]')\n",
    "for j in price:\n",
    "    Price.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make data frame\n",
    "Gaming_Laptops=pd.DataFrame({\"Laptop Model\":Laptop_model,\"Price\":Price, \"OS\":OS,\"Display\":Display,\"Memory\":Memory,\"Processor\":Processor,\n",
    "                 \"Weight\":Weight,\"Dimension\":Dimension,\"Graphical processor\":Graphics_processor})\n",
    "print('\\033[1m'+'Best Gaming Laptop :'+'\\033[0m')\n",
    "Gaming_Laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad93015",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: \n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aaadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.forbes.com\"\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on explore button to expand menu\n",
    "driver.find_element_by_xpath('//button[@class=\"icon--hamburger\"]').click()\n",
    "\n",
    "# Clicking on Billionaire category\n",
    "driver.find_element_by_xpath('//ul[@class=\"header__channels\"]/li[1]').click()\n",
    "\n",
    "# Clicking on world billionaire tab\n",
    "driver.find_element_by_xpath('//ul[@class=\"header__channels\"]/li[1]/div[2]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ee4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to scrap data\n",
    "Rank =[]\n",
    "Name=[]\n",
    "Net_worth=[]\n",
    "Age=[]\n",
    "Citizenship=[]\n",
    "Source=[]\n",
    "Industry=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Rank No\n",
    "rank=driver.find_elements_by_xpath('//div[@class=\"rank\"]')\n",
    "for i in rank:\n",
    "    Rank.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175809fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extacting name\n",
    "name=driver.find_elements_by_xpath('//div[@class=\"personName\"]/div')\n",
    "for i in name:\n",
    "    Name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88785df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Net worth\n",
    "net=driver.find_elements_by_xpath('//div[@class=\"netWorth\"]/div')\n",
    "for i in net:\n",
    "    Net_worth.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Age\n",
    "age=driver.find_elements_by_xpath('//div[@class=\"age\"]/div')\n",
    "for i in age:\n",
    "    Age.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78e810",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Extracting Citizenship\n",
    "citizen=driver.find_elements_by_xpath('//div[@class=\"countryOfCitizenship\"]')\n",
    "for i in citizen:\n",
    "    Citizenship.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Source\n",
    "try:\n",
    "    source=driver.find_elements_by_xpath('//div[@class=\"source-column\"]')\n",
    "    for i in source:\n",
    "        Source.append(i.text)\n",
    "except:\n",
    "    source=driver.find_elements_by_xpath('//*[@id=\"jeff-bezos\"]/div[6]/div/div[1]')\n",
    "    Source.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27383f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Industry\n",
    "industry=driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "for i in industry:\n",
    "    Industry.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2856e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "Forbes_list=pd.DataFrame({'Rank':Rank,'Name':Name,'Net Worth':Net_worth,'Age':Age,'Citizenship':Citizenship,\n",
    "                'Source':Source,'Industry':Industry})\n",
    "print('\\033[1m'+' Forbes World Billionaires List Oct 2021 :'+'\\033[0m')\n",
    "Forbes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec12a75",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome()\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.youtube.com\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_xpath('//input[@id=\"search\"]') \n",
    "# Feeding input video name by user to search menu through send keys\n",
    "Search.send_keys('Healing Ragas - Sitar Tabla - Brindavan Sarang - Classical Instrumental Fusion B.Sivaramakrishna Rao')\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_xpath('//button[@id=\"search-icon-legacy\"]/yt-icon')  \n",
    "# Clicking search button\n",
    "Search_button.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on video\n",
    "link = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931acced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 15000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,1000000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to scrap data\n",
    "Comments = []\n",
    "Comment_posted_ago = []\n",
    "Timeline = []\n",
    "Likes = []\n",
    "No_of_Likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting comment on video\n",
    "comment = driver.find_elements_by_id(\"content-text\")\n",
    "time.sleep(3)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(comment):\n",
    "    if i.text is None:\n",
    "        Comments.append(\"--\")\n",
    "    else:\n",
    "        Comments.append(i.text)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aef060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time of commenting\n",
    "timeline = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "time.sleep(3)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(timeline):\n",
    "    if i.text is None:\n",
    "        Timeline.append(\"-\")\n",
    "    else:\n",
    "        Timeline.append(i.text)\n",
    "for i in tqdm(range(0,len(Timeline),2)):\n",
    "    Comment_posted_ago.append(Timeline[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5042b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Comments),len(Comment_posted_ago),len(No_of_Likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7265998",
   "metadata": {},
   "outputs": [],
   "source": [
    "YT=pd.DataFrame({})\n",
    "YT['Comments']=Comments[:500]\n",
    "YT['Comment Posted Ago']=Comment_posted_ago[:500]\n",
    "YT['No. of Likes']=No_of_Likes[:500]\n",
    "YT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3227193",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall \n",
    "reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ce762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.hostelworld.com\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffebaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"page-footer-accomodation\"]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "Search = driver.find_element_by_xpath('//input[@id=\"home-search-keywords\"]')\n",
    "\n",
    "# Sending input London in search bar\n",
    "Search.send_keys(\"London\")\n",
    "time.sleep(1)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[1]/div[2]/ul/li[2]')\n",
    "london.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# do click on search button\n",
    "Search_button = driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[2]/button')\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    #fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "    #fetching distance from city centre\n",
    "    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "    #fetch privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "    #fetching dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "    #fetching facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    #lets fetch url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #lets click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    #fetching ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    #fetching total reviews\n",
    "        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "    #fetch overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    #fetch property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca70aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "data = list(zip(hostel_name,distance,rating,reviews,over_all,pvt_prices,dorms_price,facilities,description))       \n",
    "Hostel = pd.DataFrame(data, columns = [\"Hostel name\",\"Distance from city centre\",\"ratings\",\"Total reviews\",\"Overall review\",\"Privates from price\",\"Dorms from price\",\"Facilities\",\"Property Description\"])\n",
    "print('\\033[1m'+' Hostel Available in London :'+'\\033[0m')\n",
    "Hostel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff120ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
